{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mt64XBHqJiYo"
   },
   "source": [
    "~~~markdown\n",
    "Copyright 2025 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "~~~\n",
    "\n",
    "# HeAR Event Detector Demo\n",
    "<table><tbody><tr>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/google-health/hear/blob/master/notebooks/hear_event_detector_demo.ipynb\">\n",
    "      <img alt=\"Google Colab logo\" src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" width=\"32px\"><br> Run in Google Colab\n",
    "    </a>\n",
    "  </td>  \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/google-health/hear/blob/master/notebooks/hear_event_detector_demo.ipynb\">\n",
    "      <img alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://huggingface.co/google/hear\">\n",
    "      <img alt=\"HuggingFace logo\" src=\"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\" width=\"32px\"><br> View on HuggingFace\n",
    "    </a>\n",
    "  </td>\n",
    "</tr></tbody></table>\n",
    "\n",
    "\n",
    "This Colab notebook demonstrates using the HeAR (Health Acoustic Representations) model along with the included Health Event Detectors directly from Hugging Face, to identify audio clips with relevent health sounds such as coughing, breathing or sneezing, then create and utilize embeddings from this subset of health-related audio clips.\n",
    "\n",
    "This notebook is similar to `train_data_efficient_classifier.ipynb` and also uses the small [Wikimedia Commons](https://commons.wikimedia.org/wiki/Commons:Welcome) dataset of relevant health sounds. In this example the audio files are reduced to a smaller subset of clips using the event detector to identify clips containing interesting health sounds to embed with HeAR.\n",
    "\n",
    "\n",
    "\n",
    "#### This notebook demonstrates:\n",
    "\n",
    "1.  Loading all supported Hugging Face Models (HeAR, Event Detector and Frontend).\n",
    "\n",
    "2.  Detecting 2-second clips within the [Wikimedia Commons](https://commons.wikimedia.org/wiki/Commons:Welcome) dataset with high probability of containing one or more of the supported event detection labels, then generating HeAR embedddings for these clips.\n",
    "\n",
    "3.  Finding the most similar audio files to a given query audio file based on the Cosine Similarity between the respective HeAR embeddings of the audio files.\n",
    "\n",
    "4. Optimizing the event detectors and frontend for low latency on-device usage using TFLite to support large scale feature event detection or feature generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_uZFXCSuqr1V"
   },
   "source": [
    "# Authenticate with HuggingFace, skip if you have a HF_TOKEN secret"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "l-5Tj0uqS3dI",
    "ExecuteTime": {
     "end_time": "2025-04-17T06:36:01.044383Z",
     "start_time": "2025-04-17T06:36:00.379240Z"
    }
   },
   "source": [
    "from huggingface_hub.utils import HfFolder\n",
    "\n",
    "if HfFolder.get_token() is None:\n",
    "    from huggingface_hub import notebook_login\n",
    "    notebook_login()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suvan\\PycharmProjects\\PythonProject1\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mubK5MmPQcUS"
   },
   "source": [
    "# Clone HuggingFace repository snapshot\n",
    "\n",
    "This will store the HeAR and event detector models in local cache so they can be loaded later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WjfuNYeZQJdc",
    "ExecuteTime": {
     "end_time": "2025-04-17T06:36:14.807151Z",
     "start_time": "2025-04-17T06:36:04.580149Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Keras version:\", tf.keras.__version__)\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "hugging_face_repo = \"google/hear\"\n",
    "local_snapshot_path = snapshot_download(repo_id=hugging_face_repo)\n",
    "print(f\"Saved {hugging_face_repo} to {local_snapshot_path}\\n\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "Keras version: 3.9.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 24 files: 100%|██████████| 24/24 [00:00<00:00, 11995.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved google/hear to C:\\Users\\suvan\\.cache\\huggingface\\hub\\models--google--hear\\snapshots\\9b2eb2853c426676255cc6ac5804b7f1fe8e563f\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mO-Z5BOtj3D1"
   },
   "source": [
    "# Download Audio Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBZnJwzzj3D1"
   },
   "source": [
    " Wiki Commons\n",
    "https://commons.wikimedia.org/wiki/Category:Coughing_audio\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tJ55XsJsj3D2",
    "ExecuteTime": {
     "end_time": "2025-04-17T06:36:23.639914Z",
     "start_time": "2025-04-17T06:36:19.400657Z"
    }
   },
   "source": [
    "# @title Download Public Domain Cough Examples to Notebook\n",
    "import os\n",
    "import subprocess\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# More examples: https://commons.wikimedia.org/wiki/Category:Coughing_audio\n",
    "wiki_cough_file_urls = [\n",
    "  'https://upload.wikimedia.org/wikipedia/commons/c/cc/Man_coughing.ogg',\n",
    "  'https://upload.wikimedia.org/wikipedia/commons/6/6a/Cough_1.ogg',\n",
    "  'https://upload.wikimedia.org/wikipedia/commons/d/d9/Cough_2.ogg',\n",
    "  'https://upload.wikimedia.org/wikipedia/commons/b/be/Woman_coughing_three_times.wav',\n",
    "  'https://upload.wikimedia.org/wikipedia/commons/d/d0/Sneezing.ogg',\n",
    "  'https://upload.wikimedia.org/wikipedia/commons/e/ef/Laughter_and_clearing_voice.ogg',\n",
    "  'https://upload.wikimedia.org/wikipedia/commons/c/c6/Laughter.ogg',\n",
    "  'https://upload.wikimedia.org/wikipedia/commons/1/1c/Knocking_on_wood_or_door.ogg',\n",
    "]\n",
    "\n",
    "# Download the files.\n",
    "files_map = {}  # file name to file path map\n",
    "for url in wiki_cough_file_urls:\n",
    "  filename = os.path.basename(urlparse(url).path)\n",
    "  print(f'Downloading {filename}...')\n",
    "  res = subprocess.run(['wget', '-nv', '-O', filename, url], capture_output=True, text=True)\n",
    "  if res.returncode != 0:\n",
    "      print(f\"  Download failed. Return code: {res.returncode}\\nError: {res.stderr}\")\n",
    "  files_map[filename] = url\n",
    "print(f'\\nLocal Files:\\n{os.listdir():}\\n')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Man_coughing.ogg...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 23\u001B[0m\n\u001B[0;32m     21\u001B[0m filename \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mbasename(urlparse(url)\u001B[38;5;241m.\u001B[39mpath)\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDownloading \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfilename\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m...\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 23\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43msubprocess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mwget\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m-nv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m-O\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcapture_output\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m res\u001B[38;5;241m.\u001B[39mreturncode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     25\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m  Download failed. Return code: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mres\u001B[38;5;241m.\u001B[39mreturncode\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mError: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mres\u001B[38;5;241m.\u001B[39mstderr\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\subprocess.py:503\u001B[0m, in \u001B[0;36mrun\u001B[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001B[0m\n\u001B[0;32m    500\u001B[0m     kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstdout\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m PIPE\n\u001B[0;32m    501\u001B[0m     kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstderr\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m PIPE\n\u001B[1;32m--> 503\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Popen(\u001B[38;5;241m*\u001B[39mpopenargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;28;01mas\u001B[39;00m process:\n\u001B[0;32m    504\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    505\u001B[0m         stdout, stderr \u001B[38;5;241m=\u001B[39m process\u001B[38;5;241m.\u001B[39mcommunicate(\u001B[38;5;28minput\u001B[39m, timeout\u001B[38;5;241m=\u001B[39mtimeout)\n",
      "File \u001B[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\subprocess.py:971\u001B[0m, in \u001B[0;36mPopen.__init__\u001B[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001B[0m\n\u001B[0;32m    967\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtext_mode:\n\u001B[0;32m    968\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstderr \u001B[38;5;241m=\u001B[39m io\u001B[38;5;241m.\u001B[39mTextIOWrapper(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstderr,\n\u001B[0;32m    969\u001B[0m                     encoding\u001B[38;5;241m=\u001B[39mencoding, errors\u001B[38;5;241m=\u001B[39merrors)\n\u001B[1;32m--> 971\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execute_child\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexecutable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreexec_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclose_fds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    972\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mpass_fds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcwd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    973\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mstartupinfo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreationflags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshell\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    974\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mp2cread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp2cwrite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    975\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mc2pread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc2pwrite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    976\u001B[0m \u001B[43m                        \u001B[49m\u001B[43merrread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrwrite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    977\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mrestore_signals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    978\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mgid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mumask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    979\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mstart_new_session\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    980\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[0;32m    981\u001B[0m     \u001B[38;5;66;03m# Cleanup if the child failed starting.\u001B[39;00m\n\u001B[0;32m    982\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mfilter\u001B[39m(\u001B[38;5;28;01mNone\u001B[39;00m, (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstdin, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstdout, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstderr)):\n",
      "File \u001B[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\subprocess.py:1456\u001B[0m, in \u001B[0;36mPopen._execute_child\u001B[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001B[0m\n\u001B[0;32m   1454\u001B[0m \u001B[38;5;66;03m# Start the process\u001B[39;00m\n\u001B[0;32m   1455\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1456\u001B[0m     hp, ht, pid, tid \u001B[38;5;241m=\u001B[39m \u001B[43m_winapi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCreateProcess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexecutable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1457\u001B[0m \u001B[43m                             \u001B[49m\u001B[38;5;66;43;03m# no special security\u001B[39;49;00m\n\u001B[0;32m   1458\u001B[0m \u001B[43m                             \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1459\u001B[0m \u001B[43m                             \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mclose_fds\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1460\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mcreationflags\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1461\u001B[0m \u001B[43m                             \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1462\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mcwd\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1463\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mstartupinfo\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1464\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m   1465\u001B[0m     \u001B[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001B[39;00m\n\u001B[0;32m   1466\u001B[0m     \u001B[38;5;66;03m# handles that only the child should have open.  You need\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1469\u001B[0m     \u001B[38;5;66;03m# pipe will not close when the child process exits and the\u001B[39;00m\n\u001B[0;32m   1470\u001B[0m     \u001B[38;5;66;03m# ReadFile will hang.\u001B[39;00m\n\u001B[0;32m   1471\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001B[0;32m   1472\u001B[0m                          c2pread, c2pwrite,\n\u001B[0;32m   1473\u001B[0m                          errread, errwrite)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2Eyu4VIsbUB"
   },
   "source": [
    "# Load Models and Run Inference\n",
    "\n",
    "### HeAR Model\n",
    "\n",
    "The HeAR model uses a powerful [ViT](https://huggingface.co/docs/transformers/en/model_doc/vit) backbone and generates rich 512 length embeddings from a 2 second single channel 16kHz audio clip. See the [HuggingFace Model Card](https://huggingface.co/google/hear) for more details.\n",
    "\n",
    "### Event Detector Models\n",
    "\n",
    "The event detector models use the efficient [MobileNet-V3](https://huggingface.co/docs/timm/en/models/mobilenet-v3) backbone paired with our custom TensorFlow spectrogram frontend.\n",
    "\n",
    "As with HeAR, these models expect a 2 second single channel 16kHz audio clip and output 8 detection probability scores for the following labels:\n",
    "\n",
    "```\n",
    "['Cough', 'Snore', 'Baby Cough', 'Breathe', 'Sneeze','Throat Clear', 'Laugh', 'Speech']\n",
    "```\n",
    "\n",
    "The event detector has two size variants which can be used interchangeably depending on the use-case.\n",
    "\n",
    "*  `event_detector_small/` is based on [MobileNetV3Small](https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV3Small) with approximately **1M** parameters (3.60 MB)\n",
    "\n",
    "*  `event_detector_large/` is based on [MobileNetV3Large](https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV3Large) with approximately **3M** parameters (11.46 MB)\n",
    "\n",
    "#### Spectrogram Frontend\n",
    "\n",
    "Our event detectors are fused with a custom, on-device optimized spectrogram frontend which efficiently converts 2 seconds of 16kHz audio into [PCEN](https://research.google/pubs/trainable-frontend-for-robust-and-far-field-keyword-spotting) scaled [Mel-spectrogram](https://huggingface.co/learn/audio-course/en/chapter1/audio_data#mel-spectrogram) features with 200 time steps and 48 Mel-frequency bins.\n",
    "\n",
    "*  `spectrogram_frontend/` is based on the PCEN implementation from [LEAF](https://research.google/blog/leaf-a-learnable-frontend-for-audio-classification/) and has only **5k** non-trainable parameters (18.56 KB) which are frozen and not configurable.\n",
    "\n",
    "We provide a standalone version of this frontend so that features can be pre-computed for new event detector training applications. See **Extract Batch Frontend Spectrogram Features** section for usage examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cQVAxkBHZ_JO",
    "ExecuteTime": {
     "end_time": "2025-04-17T06:36:50.802068Z",
     "start_time": "2025-04-17T06:36:43.593325Z"
    }
   },
   "source": [
    "# @title Load HeAR, Event Detector and Frontend Models\n",
    "from huggingface_hub import from_pretrained_keras\n",
    "\n",
    "# Constants for all included models, each input should be 32,000 samples.\n",
    "SAMPLE_RATE = 16000\n",
    "CLIP_DURATION = 2\n",
    "\n",
    "# Select event detector variant\n",
    "EVENT_DETECTOR = \"event_detector_small\" # @param [\"event_detector_large\", \"event_detector_small\"]\n",
    "# Included event detectors are trained to doutput detection probabilities in this order.\n",
    "LABEL_LIST =  ['Cough', 'Snore', 'Baby Cough', 'Breathe', 'Sneeze', 'Throat Clear', 'Laugh', 'Speech']\n",
    "\n",
    "# HeAR Embedding Model\n",
    "print(f\"\\nLoading HeAR model\")\n",
    "hear_model = from_pretrained_keras(local_snapshot_path)\n",
    "hear_infer = hear_model.signatures[\"serving_default\"]\n",
    "\n",
    "# Event detector models and frontend are nested in the \"event_detector/\" folder.\n",
    "# Detector Frontend Model for efficiently computing spectrogram feature\n",
    "frontend_path = os.path.join(\"event_detector/\", \"spectrogram_frontend\")\n",
    "print(f\"\\nLoading frontend model from: {frontend_path}\")\n",
    "frontend_model = from_pretrained_keras(\n",
    "    os.path.join(local_snapshot_path, frontend_path)\n",
    ")\n",
    "\n",
    "# Detector Model based on size variant selection, frontend included\n",
    "event_detector_path = os.path.join(\"event_detector/\", EVENT_DETECTOR)\n",
    "print(f\"\\nLoading detector model from: {event_detector_path}\")\n",
    "event_detector = from_pretrained_keras(\n",
    "    os.path.join(local_snapshot_path, event_detector_path)\n",
    ")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json not found in C:\\Users\\suvan\\.cache\\huggingface\\hub\\models--google--hear\\snapshots\\9b2eb2853c426676255cc6ac5804b7f1fe8e563f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading HeAR model\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "File format not supported: filepath=C:\\Users\\suvan\\.cache\\huggingface\\hub\\models--google--hear\\snapshots\\9b2eb2853c426676255cc6ac5804b7f1fe8e563f. Keras 3 only supports V3 `.keras` files and legacy H5 format files (`.h5` extension). Note that the legacy SavedModel format is not supported by `load_model()` in Keras 3. In order to reload a TensorFlow SavedModel as an inference-only layer in Keras 3, use `keras.layers.TFSMLayer(C:\\Users\\suvan\\.cache\\huggingface\\hub\\models--google--hear\\snapshots\\9b2eb2853c426676255cc6ac5804b7f1fe8e563f, call_endpoint='serving_default')` (note that your `call_endpoint` might have a different name).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 15\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# HeAR Embedding Model\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mLoading HeAR model\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 15\u001B[0m hear_model \u001B[38;5;241m=\u001B[39m \u001B[43mfrom_pretrained_keras\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlocal_snapshot_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m hear_infer \u001B[38;5;241m=\u001B[39m hear_model\u001B[38;5;241m.\u001B[39msignatures[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mserving_default\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# Event detector models and frontend are nested in the \"event_detector/\" folder.\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# Detector Frontend Model for efficiently computing spectrogram feature\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\PythonProject1\\.venv\\lib\\site-packages\\huggingface_hub\\keras_mixin.py:293\u001B[0m, in \u001B[0;36mfrom_pretrained_keras\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    240\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mfrom_pretrained_keras\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mKerasModelHubMixin\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    241\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    242\u001B[0m \u001B[38;5;124;03m    Instantiate a pretrained Keras model from a pre-trained model from the Hub.\u001B[39;00m\n\u001B[0;32m    243\u001B[0m \u001B[38;5;124;03m    The model is expected to be in `SavedModel` format.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    291\u001B[0m \u001B[38;5;124;03m    </Tip>\u001B[39;00m\n\u001B[0;32m    292\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 293\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m KerasModelHubMixin\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\PythonProject1\\.venv\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m check_use_auth_token:\n\u001B[0;32m    112\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[1;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\PythonProject1\\.venv\\lib\\site-packages\\huggingface_hub\\hub_mixin.py:566\u001B[0m, in \u001B[0;36mModelHubMixin.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, force_download, resume_download, proxies, token, cache_dir, local_files_only, revision, **model_kwargs)\u001B[0m\n\u001B[0;32m    563\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_hub_mixin_inject_config \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfig\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m model_kwargs:\n\u001B[0;32m    564\u001B[0m         model_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfig\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m config\n\u001B[1;32m--> 566\u001B[0m instance \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_from_pretrained(\n\u001B[0;32m    567\u001B[0m     model_id\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mstr\u001B[39m(model_id),\n\u001B[0;32m    568\u001B[0m     revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[0;32m    569\u001B[0m     cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[0;32m    570\u001B[0m     force_download\u001B[38;5;241m=\u001B[39mforce_download,\n\u001B[0;32m    571\u001B[0m     proxies\u001B[38;5;241m=\u001B[39mproxies,\n\u001B[0;32m    572\u001B[0m     resume_download\u001B[38;5;241m=\u001B[39mresume_download,\n\u001B[0;32m    573\u001B[0m     local_files_only\u001B[38;5;241m=\u001B[39mlocal_files_only,\n\u001B[0;32m    574\u001B[0m     token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[0;32m    575\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[0;32m    576\u001B[0m )\n\u001B[0;32m    578\u001B[0m \u001B[38;5;66;03m# Implicitly set the config as instance attribute if not already set by the class\u001B[39;00m\n\u001B[0;32m    579\u001B[0m \u001B[38;5;66;03m# This way `config` will be available when calling `save_pretrained` or `push_to_hub`.\u001B[39;00m\n\u001B[0;32m    580\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m config \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mgetattr\u001B[39m(instance, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_hub_mixin_config\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;28;01mNone\u001B[39;00m, {})):\n",
      "File \u001B[1;32m~\\PycharmProjects\\PythonProject1\\.venv\\lib\\site-packages\\huggingface_hub\\keras_mixin.py:495\u001B[0m, in \u001B[0;36mKerasModelHubMixin._from_pretrained\u001B[1;34m(cls, model_id, revision, cache_dir, force_download, proxies, resume_download, local_files_only, token, config, **model_kwargs)\u001B[0m\n\u001B[0;32m    492\u001B[0m     storage_folder \u001B[38;5;241m=\u001B[39m model_id\n\u001B[0;32m    494\u001B[0m \u001B[38;5;66;03m# TODO: change this in a future PR. We are not returning a KerasModelHubMixin instance here...\u001B[39;00m\n\u001B[1;32m--> 495\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstorage_folder\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    497\u001B[0m \u001B[38;5;66;03m# For now, we add a new attribute, config, to store the config loaded from the hub/a local dir.\u001B[39;00m\n\u001B[0;32m    498\u001B[0m model\u001B[38;5;241m.\u001B[39mconfig \u001B[38;5;241m=\u001B[39m config\n",
      "File \u001B[1;32m~\\PycharmProjects\\PythonProject1\\.venv\\lib\\site-packages\\keras\\src\\saving\\saving_api.py:206\u001B[0m, in \u001B[0;36mload_model\u001B[1;34m(filepath, custom_objects, compile, safe_mode)\u001B[0m\n\u001B[0;32m    200\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    201\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFile not found: filepath=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfilepath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    202\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease ensure the file is an accessible `.keras` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    203\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mzip file.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    204\u001B[0m     )\n\u001B[0;32m    205\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 206\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    207\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFile format not supported: filepath=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfilepath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    208\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mKeras 3 only supports V3 `.keras` files and \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    209\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlegacy H5 format files (`.h5` extension). \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    210\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNote that the legacy SavedModel format is not \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    211\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msupported by `load_model()` in Keras 3. In \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    212\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morder to reload a TensorFlow SavedModel as an \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    213\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minference-only layer in Keras 3, use \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    214\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`keras.layers.TFSMLayer(\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    215\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfilepath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, call_endpoint=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mserving_default\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m)` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    216\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(note that your `call_endpoint` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    217\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmight have a different name).\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    218\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: File format not supported: filepath=C:\\Users\\suvan\\.cache\\huggingface\\hub\\models--google--hear\\snapshots\\9b2eb2853c426676255cc6ac5804b7f1fe8e563f. Keras 3 only supports V3 `.keras` files and legacy H5 format files (`.h5` extension). Note that the legacy SavedModel format is not supported by `load_model()` in Keras 3. In order to reload a TensorFlow SavedModel as an inference-only layer in Keras 3, use `keras.layers.TFSMLayer(C:\\Users\\suvan\\.cache\\huggingface\\hub\\models--google--hear\\snapshots\\9b2eb2853c426676255cc6ac5804b7f1fe8e563f, call_endpoint='serving_default')` (note that your `call_endpoint` might have a different name)."
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33G4zKJHjGGc"
   },
   "outputs": [],
   "source": [
    "# @title Plot Helpers\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "from IPython.display import Audio\n",
    "import matplotlib.cm as cm\n",
    "import warnings\n",
    "\n",
    "# Suppress the specific warning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"soundfile\")\n",
    "warnings.filterwarnings(\"ignore\", module=\"librosa\")\n",
    "\n",
    "def plot_waveform(sound, sr, title, figsize=(12, 4), color='blue', alpha=0.7):\n",
    "  \"\"\"Plots the waveform of the audio using librosa.display.\"\"\"\n",
    "  plt.figure(figsize=figsize)\n",
    "  librosa.display.waveshow(sound, sr=sr, color=color, alpha=alpha)\n",
    "  plt.title(f\"{title}\\nshape={sound.shape}, sr={sr}, dtype={sound.dtype}\")\n",
    "  plt.xlabel(\"Time (s)\")\n",
    "  plt.ylabel(\"Amplitude\")\n",
    "  plt.grid(True)\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_spectrogram(sound, sr, title, figsize=(12, 4), n_fft=2048, hop_length=256, n_mels=128, cmap='nipy_spectral'):\n",
    "  \"\"\"Plots the Mel spectrogram of the audio using librosa.\"\"\"\n",
    "  plt.figure(figsize=figsize)\n",
    "  mel_spectrogram = librosa.feature.melspectrogram(y=sound, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "  log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "  librosa.display.specshow(log_mel_spectrogram, sr=sr, hop_length=hop_length, x_axis='time', y_axis='mel', cmap=cmap)\n",
    "  plt.title(f\"{title} - Mel Spectrogram\")\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3dJKyHXznPgg"
   },
   "outputs": [],
   "source": [
    "# @title Event Detector Plot Helpers\n",
    "\n",
    "def plot_frontend_feature(\n",
    "    frontend_feature: np.ndarray,\n",
    "    title: str,\n",
    "    figsize: tuple[int, int] = (12, 4),\n",
    "    cmap: str = 'nipy_spectral',\n",
    ") -> None:\n",
    "  \"\"\"Plots the frontend spectrogram input feature.\n",
    "\n",
    "  Args:\n",
    "    frontend_feature: The event detector frontend feature as a 2D NumPy array\n",
    "      with shape (number of time steps, number of frequency bins). The default\n",
    "      shape for the included event detectors is (200, 48), which represents a\n",
    "      2-second audio clip with 48 frequency bins.\n",
    "    title: The title prefix of the plot.\n",
    "    figsize: Optional size of the figure.\n",
    "    cmap: Optional colormap to use.\n",
    "  \"\"\"\n",
    "  # Frontend features are typically rotated when fed into the model\n",
    "  # for spectrogram visualization it is more standard for x axis to be time\n",
    "  audio_spectrogram = np.rot90(frontend_feature)\n",
    "  plt.figure(figsize=figsize)\n",
    "  plt.imshow(audio_spectrogram, aspect='auto', cmap=cmap)\n",
    "  plt.title(f\"{title} - Frontend PCEN Mel Spectrogram\")\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "def plot_detection_scores(\n",
    "    scores_batch: np.ndarray,\n",
    "    label_list: list[str],\n",
    "    title: str,\n",
    "    figsize: tuple[int, int] = (12, 4),\n",
    "    cmap: str = 'nipy_spectral',\n",
    ") -> None:\n",
    "  \"\"\"Plots per-label detection scores for batch sequentially with a consistent color scale.\n",
    "\n",
    "  Args:\n",
    "    scores_batch: The event detection scores as a 2D NumPy array with shape\n",
    "      (number of clips, number of labels). Where number of labels should match\n",
    "      the length of the label_list, which is 8 for the included event detectors.\n",
    "    label_list: A list of labels representing the event detector classes.\n",
    "    title: The title prefix of the plot.\n",
    "    figsize: Optional size of the figure.\n",
    "    cmap: Optional colormap to use.\n",
    "  \"\"\"\n",
    "  plt.figure(figsize=figsize)\n",
    "  scores_img = np.transpose(scores_batch)\n",
    "  # Explicitly set the color limits for imshow\n",
    "  im = plt.imshow(scores_img, aspect='auto', cmap=cmap, vmin=0, vmax=1)\n",
    "  # Set up the 'y' label axis\n",
    "  plt.yticks(\n",
    "      np.arange(len(label_list)), [l.replace(' ', '\\n') for l in label_list]\n",
    "  )\n",
    "  # Add horizontal grid lines between labels\n",
    "  for i in range(1, scores_img.shape[0]):\n",
    "    plt.axhline(y=i - 0.5, color='gray', linestyle='--')\n",
    "  plt.grid(axis='y', which='major', color='white', alpha=0)\n",
    "  # Setup the 'x' time axis\n",
    "  n_clips = scores_img.shape[1]\n",
    "  plt.xticks(np.arange(n_clips), [f'Clip {i+1}' for i in range(n_clips)])\n",
    "  plt.xlabel(\"Time Step\")\n",
    "  # Add vertical grid lines between time steps\n",
    "  for j in range(1, n_clips):\n",
    "    plt.axvline(x=j - 0.5, color='gray', linestyle='--')\n",
    "  plt.title(f\"{title} - Sound Event Detections\")\n",
    "  # Add colorbar with a consistent scale from 0 to 1\n",
    "  plt.colorbar(im, ticks=[0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYbz904QXio2"
   },
   "outputs": [],
   "source": [
    "# @title Load Audio and Generate HeAR Embeddings\n",
    "%%time\n",
    "\n",
    "# Audio display options\n",
    "SHOW_WAVEFORM = False\n",
    "SHOW_SPECTROGRAM = False\n",
    "SHOW_PLAYER = True\n",
    "SHOW_DETECTION_SCORES = True\n",
    "COLORMAP = \"Blues\"\n",
    "\n",
    "# Keep clips with high detection scores for these respiratory labels\n",
    "# then embed the clips using HeAR. Ignore clips with no detections\n",
    "LABELS_TO_EMBED = ['Cough', 'Snore', 'Breathe', 'Sneeze']\n",
    "# Assert that all labels to embed are actually present in the main label list\n",
    "assert all(label in LABEL_LIST for label in LABELS_TO_EMBED)\n",
    "\n",
    "# Clips of length CLIP_DURATION seconds are extracted from the audio file\n",
    "# using a sliding window. Adjecent clips are overlapped by CLIP_OVERLAP_PERCENT.\n",
    "CLIP_OVERLAP_PERCENT = 10\n",
    "\n",
    "# Labels must have score above this threshold to be considered a detection\n",
    "DETECTION_THRESHOLD = 0.9\n",
    "\n",
    "frame_length = int(CLIP_DURATION * SAMPLE_RATE)\n",
    "frame_step = int(frame_length * (1 - CLIP_OVERLAP_PERCENT / 100))\n",
    "hear_embeddings = {}\n",
    "for file_key, file_url in files_map.items():\n",
    "  hear_embeddings[file_key] = {}\n",
    "  print(f\"\\nLoading file: {file_key} from {file_url}\")\n",
    "  audio, sample_rate = librosa.load(file_key, sr=SAMPLE_RATE, mono=True)\n",
    "\n",
    "  # Display full audio file (optional).\n",
    "  if SHOW_WAVEFORM:\n",
    "    plot_waveform(audio, sample_rate, title=file_key, color='blue')\n",
    "  if SHOW_SPECTROGRAM:\n",
    "    plot_spectrogram(\n",
    "      audio, sample_rate, title=file_key, n_fft=2*1024, hop_length=64, n_mels=256, cmap=COLORMAP)\n",
    "  if SHOW_PLAYER:\n",
    "    display(Audio(data=audio, rate=sample_rate))\n",
    "\n",
    "  # Segment an audio array into fixed length overlapping clips.\n",
    "  if len(audio) < frame_length:\n",
    "    audio = np.pad(audio, (0, frame_length - len(audio)), mode='constant')\n",
    "  audio_clip_batch = tf.signal.frame(audio, frame_length, frame_step )\n",
    "  print(f\"Number of audio clips in batch: {len(audio_clip_batch)}.\")\n",
    "\n",
    "  # Perform detector inference on the audio_clip_batch\n",
    "  # The model will generate the input feature, then infer the detection\n",
    "  print(f\"Running batched {EVENT_DETECTOR} model inference on audio clips.\")\n",
    "  detection_scores_batch = event_detector(audio_clip_batch)[\"scores\"].numpy()\n",
    "  print(\"Computed batch probability scores with shape:\",\n",
    "        f\"{detection_scores_batch.shape} from input audio clips batch with\",\n",
    "        f\"shape: {audio_clip_batch.shape}\"\n",
    "  )\n",
    "  hear_embeddings[file_key]['detections'] = detection_scores_batch\n",
    "\n",
    "  if SHOW_DETECTION_SCORES:\n",
    "    plot_detection_scores(detection_scores_batch, LABEL_LIST, title=f'{file_key}: {EVENT_DETECTOR}', cmap=COLORMAP)\n",
    "\n",
    "  # Filter clips for HeAR inference based on if in ANY detection scores for\n",
    "  # 'LABELS_TO_EMBED' are above the 'DETECTION_THRESHOLD'.\n",
    "  print(f\"Filtering clips based on detections for labels: {LABELS_TO_EMBED}\")\n",
    "  embed_hear_clips = []\n",
    "  for clip_i, scores in enumerate(detection_scores_batch):\n",
    "    for label_index, label in enumerate(LABEL_LIST):\n",
    "      if label in LABELS_TO_EMBED and scores[label_index] > DETECTION_THRESHOLD:\n",
    "        embed_hear_clips.append(audio_clip_batch[clip_i])\n",
    "        break\n",
    "\n",
    "  # Perform HeAR batch inference to extract the associated clip embedding.\n",
    "  # Only run inference on 'embed_hear_clips' which have have a high detection\n",
    "  # score for one of the 'LABELS_TO_EMBED'.\n",
    "  if len(embed_hear_clips):\n",
    "    print(f\"Computing HeAR embedding for batch of\",\n",
    "          f\"{len(embed_hear_clips)} selected clips.\")\n",
    "    hear_embedding_batch = hear_infer(x=np.asarray(embed_hear_clips))[ 'output_0'].numpy()\n",
    "    print(f\"Embedding batch shape: {hear_embedding_batch.shape},\",\n",
    "          f\"data type: {hear_embedding_batch.dtype}\")\n",
    "  else:\n",
    "    hear_embedding_batch = np.array([])\n",
    "    print(f\"None of the {len(audio_clip_batch)} clips in {file_key} have\",\n",
    "          f\"detections above the threshold: {DETECTION_THRESHOLD} for the\",\n",
    "          f\"labels: {LABELS_TO_EMBED}\")\n",
    "  hear_embeddings[file_key]['embeddings'] = hear_embedding_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PFLdq8WI3AJT"
   },
   "outputs": [],
   "source": [
    "# @title Use HeAR embeddings to find most similar file to query file\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Set up query file and make sure if the query file_key exists in the dictionary and has embeddings.\n",
    "query_file_key = 'Cough_1.ogg'\n",
    "assert query_file_key in hear_embeddings and len(hear_embeddings[query_file_key]['embeddings'])\n",
    "\n",
    "# Get the average embedding for the query file and compare similarity to the average embedding for the other files.\n",
    "query_embedding = np.mean(hear_embeddings[query_file_key]['embeddings'], axis=0)\n",
    "similarities = {}\n",
    "for file_key, model_outputs in hear_embeddings.items():\n",
    "  # Skip comparing file_key to itself or comparing to keys without HeAR embeddings.\n",
    "  if file_key == query_file_key or not len(model_outputs['embeddings']):\n",
    "    continue\n",
    "\n",
    "  # Compute cosine similarity between the query file and the current file.\n",
    "  current_embedding = np.mean(model_outputs['embeddings'], axis=0)\n",
    "  similarities[file_key] = 1 - distance.cosine(query_embedding, current_embedding)\n",
    "\n",
    "# Find the top N most similar entries\n",
    "N = 3\n",
    "top_N_similar = dict(sorted(similarities.items(), key=lambda item: item[1], reverse=True)[:N])\n",
    "print(f\"\\nTop {N} most similar entries to '{query_file_key}':\")\n",
    "for key, similarity in top_N_similar.items():\n",
    "    print(f\"  {key}: {similarity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4LoPrUCs0F4"
   },
   "source": [
    "# Extract Batch Frontend Spectrogram Features\n",
    "\n",
    "We provide a standalone, frozen frontend spectrogram model for efficient generation of [PCEN Mel-Spectrogram](https://research.google/pubs/trainable-frontend-for-robust-and-far-field-keyword-spotting/) features, which are used by the event detectors. This model is comprised of non-trainable TensorFlow operations, prioritizing portability, scalability, and optimization at the cost of configurability.\n",
    "\n",
    "The frontend can also be used as a standalone feature extractor for generating large amounts of training data for finetuning or retraining additional models as demonstrated below.\n",
    "\n",
    "The input must be 2-seconds and corresponding output feature will have shape `(200, 48)`. Typically, input clips will be segmented with some amount of overlap to avoid distorting sounds near the boundry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DLDc2OFAngcT"
   },
   "outputs": [],
   "source": [
    "# @title Plot Example Frontend Features\n",
    "example_file_key =  'Sneezing.ogg'\n",
    "audio, sample_rate = librosa.load(example_file_key, sr=SAMPLE_RATE, mono=True)\n",
    "print(f\"Loaded {example_file_key} with duration {len(audio)/sample_rate:0.2f}s\")\n",
    "\n",
    "# Extract example clips with 50% overlap, then generate frontend feature batch.\n",
    "frame_length = int(CLIP_DURATION * SAMPLE_RATE)\n",
    "audio_clip_batch = tf.signal.frame(audio, frame_length, frame_length // 2 )\n",
    "frontend_feature_batch = frontend_model(audio_clip_batch)\n",
    "print(f\"Generated input feature batch from {example_file_key} with shape: {frontend_feature_batch.shape}\")\n",
    "for clip_i, frontend_feature in enumerate(frontend_feature_batch):\n",
    "  plot_frontend_feature(frontend_feature.numpy(), title=f'Clip: {clip_i}', cmap=COLORMAP, figsize=(8, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9e_m3Wf0ClK"
   },
   "source": [
    "# Convert Event Detectors to TFLite\n",
    "\n",
    "The included event detectors are based on [MobileNet-V3](https://huggingface.co/docs/timm/en/models/mobilenet-v3) and are designed to be run on-device with low latency and power requirements while also allowing for finetuning and quick training.\n",
    "\n",
    "Converting the event detector models to [LiteRT](https://ai.google.dev/edge/litert) (formally TFLite) allows for more optimal performance on specific hardware such as mobile devices or compute constrained servers.\n",
    "\n",
    "In order to save compute in realtime or large scale applications, these hardware optimized event detectors can be used as a gate for HeAR, only embedding clips detected to contain a health sound of interest (as shown above).\n",
    "\n",
    "The code below demonstrates converting the loaded event detector to a TensorFlow LiteRT model, there are many more options available to quantize or furthur optimize the converted models, see the [documentation](https://ai.google.dev/edge/litert/models/convert_tf) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5UimMm15FMy"
   },
   "outputs": [],
   "source": [
    "# @title TFLite Conversion\n",
    "%%time\n",
    "\n",
    "def convert_to_tflite(\n",
    "    model: tf.keras.Model,\n",
    "    quantize: bool = False,\n",
    ") -> bytes:\n",
    "  \"\"\"Converts a SavedModel model to a TensorFlow Lite (TFLite) model.\n",
    "\n",
    "  Args:\n",
    "    model: The model to convert.\n",
    "    quantize: If True, apply dynamic range quantization to optimize the model.\n",
    "\n",
    "  Returns:\n",
    "    The raw byte representation of the converted TFLite model.\n",
    "  \"\"\"\n",
    "  converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "  converter.target_spec.supported_ops = [\n",
    "      tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "      tf.lite.OpsSet.SELECT_TF_OPS,  # needed for frontend ops\n",
    "  ]\n",
    "  # See documentation for quantization options beyond dyanmic range quantization.\n",
    "  # https://ai.google.dev/edge/litert/models/post_training_quantization\n",
    "  if quantize:\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "  return converter.convert()\n",
    "\n",
    "# Convert event detector to TFLite and save result.\n",
    "event_detector_lite = convert_to_tflite(event_detector, quantize=False)\n",
    "tflite_output_path ='event_detector.tflite'\n",
    "with open(tflite_output_path, 'wb') as f:\n",
    "  f.write(event_detector_lite)\n",
    "print(f\"Saved TFLite model to: {tflite_output_path}\")\n",
    "\n",
    "# Initalize TFLite Model and print I/O specification.\n",
    "tflite_interp = tf.lite.Interpreter(model_content=event_detector_lite)\n",
    "print(f\"Input details:\\n {tflite_interp.get_input_details()[0]}\")\n",
    "print(f\"Output details: \\n {tflite_interp.get_output_details()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHTxQttKYNpa"
   },
   "source": [
    "# Next steps\n",
    "\n",
    "Explore the other [notebooks](https://github.com/google-health/hear/blob/master/notebooks) to learn what else you can do with the model."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "last_runtime": {
    "build_target": "//experimental/health_foundation_models/colab:colab_deps",
    "kind": "private"
   },
   "name": "hear_event_detector_demo.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
