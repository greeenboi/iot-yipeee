{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Health Sound Analysis using HeAR Embeddings\n",
    "\n",
    "This notebook demonstrates how to use the HeAR model to analyze cough sounds and determine if a person might be sick.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Install dependencies"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "! pip install --upgrade --quiet transformers==4.50.3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Authentication with HuggingFace\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from huggingface_hub.utils import HfFolder\n",
    "\n",
    "if HfFolder.get_token() is None:\n",
    "    from huggingface_hub import notebook_login\n",
    "    notebook_login()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load and play cough audio recording"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "SAMPLE_RATE = 16000  # Samples per second (Hz)\n",
    "CLIP_DURATION = 2    # Duration of the audio clip in seconds\n",
    "CLIP_LENGTH = SAMPLE_RATE * CLIP_DURATION  # Total number of samples"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!wget -nc https://upload.wikimedia.org/wikipedia/commons/b/be/Woman_coughing_three_times.wav"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from scipy.io import wavfile\n",
    "\n",
    "# Load file\n",
    "with open('Woman_coughing_three_times.wav', 'rb') as f:\n",
    "  original_sampling_rate, audio_array = wavfile.read(f)\n",
    "\n",
    "print(f\"Sample Rate: {original_sampling_rate} Hz\")\n",
    "print(f\"Data Shape: {audio_array.shape}\")\n",
    "print(f\"Data Type: {audio_array.dtype}\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from IPython.display import Audio, display\n",
    "import importlib\n",
    "# Clone the repository if it's not already available\n",
    "! git clone https://github.com/Google-Health/hear.git 2>/dev/null || (cd hear && git pull)\n",
    "\n",
    "audio_utils = importlib.import_module(\n",
    "    \"hear.python.data_processing.audio_utils\"\n",
    ")\n",
    "resample_audio_and_convert_to_mono = audio_utils.resample_audio_and_convert_to_mono\n",
    "\n",
    "\n",
    "audio_array = resample_audio_and_convert_to_mono(\n",
    "  audio_array=audio_array, \n",
    "  sampling_rate=original_sampling_rate,\n",
    "  new_sampling_rate=SAMPLE_RATE,\n",
    ")\n",
    "display(Audio(audio_array, rate=SAMPLE_RATE))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Event Detector and HeAR Models"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from huggingface_hub import HfFolder\n",
    "from transformers import ViTConfig, ViTModel\n",
    "\n",
    "# Get your token (this should work if you've logged in with notebook_login)\n",
    "token = HfFolder.get_token()\n",
    "\n",
    "# Ensure the token is being used\n",
    "if token:\n",
    "    print(\"Token found, attempting to use it for authentication\")\n",
    "    os.environ[\"HUGGINGFACE_TOKEN\"] = token\n",
    "else:\n",
    "    print(\"No token found. Please run the notebook_login() cell again\")\n",
    "    from huggingface_hub import notebook_login\n",
    "    notebook_login()\n",
    "    token = HfFolder.get_token()\n",
    "    os.environ[\"HUGGINGFACE_TOKEN\"] = token\n",
    "\n",
    "# Constants for event detector\n",
    "LABEL_LIST = ['Cough', 'Snore', 'Baby Cough', 'Breathe', 'Sneeze', 'Throat Clear', 'Laugh', 'Speech']\n",
    "DETECTION_THRESHOLD = 0.5  # Probability threshold for detection\n",
    "\n",
    "# Load HeAR model directly using PyTorch\n",
    "print(f\"\\nLoading HeAR model\")\n",
    "configuration = ViTConfig(\n",
    "    image_size=(192, 128),\n",
    "    hidden_size=1024,\n",
    "    num_hidden_layers=24,\n",
    "    num_attention_heads=16,\n",
    "    intermediate_size=1024 * 4,\n",
    "    hidden_act=\"gelu_fast\",\n",
    "    hidden_dropout_prob=0.0,\n",
    "    attention_probs_dropout_prob=0.0,\n",
    "    initializer_range=0.02,\n",
    "    layer_norm_eps=1e-6,\n",
    "    pooled_dim=512,\n",
    "    patch_size=16,\n",
    "    num_channels=1,\n",
    "    qkv_bias=True,\n",
    "    encoder_stride=16,\n",
    "    pooler_act='linear',\n",
    "    pooler_output_size=512,\n",
    ")\n",
    "hear_model = ViTModel.from_pretrained(\n",
    "    \"google/hear-pytorch\",\n",
    "    config=configuration,\n",
    "    token=token,\n",
    ")\n",
    "hear_model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Create a utility function for processing audio with the PyTorch model\n",
    "def process_audio_with_hear(audio_array):\n",
    "    \"\"\"Process audio with the HeAR model using PyTorch\"\"\"\n",
    "    # Get the preprocessing method from the hear repo\n",
    "    preprocess_audio = audio_utils.preprocess_audio\n",
    "    \n",
    "    # Convert audio to correct format\n",
    "    if isinstance(audio_array, np.ndarray):\n",
    "        audio_tensor = torch.from_numpy(audio_array).float()\n",
    "    else:\n",
    "        audio_tensor = audio_array\n",
    "    \n",
    "    # Ensure audio is in the right shape [batch, sequence]\n",
    "    if audio_tensor.dim() == 1:\n",
    "        audio_tensor = audio_tensor.unsqueeze(0)\n",
    "    \n",
    "    # Preprocess audio using the function from the HeAR repo\n",
    "    inputs = preprocess_audio(audio_tensor)\n",
    "    \n",
    "    # Run the model\n",
    "    with torch.no_grad():\n",
    "        outputs = hear_model(inputs, return_dict=True)\n",
    "    \n",
    "    # Return the embedding vector\n",
    "    return outputs.pooler_output\n",
    "\n",
    "# Create simple event detector function using PyTorch\n",
    "# Note: This is a simplified detector until we can properly load the event detector model\n",
    "def simple_event_detector(audio_clips, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Simple event detector based on audio energy\n",
    "    Returns mock scores for demonstration purposes\n",
    "    \"\"\"\n",
    "    batch_size = len(audio_clips)\n",
    "    scores = np.zeros((batch_size, len(LABEL_LIST)))\n",
    "    \n",
    "    for i, clip in enumerate(audio_clips):\n",
    "        # Calculate energy-based features\n",
    "        energy = np.mean(np.abs(clip))\n",
    "        zero_crossings = np.sum(np.diff(np.signbit(clip).astype(int)))\n",
    "        \n",
    "        # Simple heuristics (just for demonstration)\n",
    "        if energy > 0.05 and zero_crossings > 100:\n",
    "            # Higher probability for cough\n",
    "            scores[i, 0] = 0.85  # Cough\n",
    "            scores[i, 4] = 0.65  # Sneeze\n",
    "        elif energy > 0.02:\n",
    "            # Higher probability for breathe\n",
    "            scores[i, 3] = 0.75  # Breathe\n",
    "        else:\n",
    "            # Background noise\n",
    "            scores[i, -1] = 0.6  # Speech (as fallback)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "print(\"Models loaded successfully!\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Utility Functions for Visualization"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "from IPython.display import Audio\n",
    "import warnings\n",
    "\n",
    "# Suppress common warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"soundfile\")\n",
    "warnings.filterwarnings(\"ignore\", module=\"librosa\")\n",
    "\n",
    "def plot_waveform(sound, sr, title, figsize=(12, 4), color='blue', alpha=0.7):\n",
    "    \"\"\"Plots the waveform of the audio\"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    librosa.display.waveshow(sound, sr=sr, color=color, alpha=alpha)\n",
    "    plt.title(f\"{title}\\nshape={sound.shape}, sr={sr}, dtype={sound.dtype}\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_detection_scores(scores_batch, label_list, title, figsize=(12, 4), cmap='Blues'):\n",
    "    \"\"\"Plots per-label detection scores for batch\"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    scores_img = np.transpose(scores_batch)\n",
    "    # Explicitly set the color limits for imshow\n",
    "    im = plt.imshow(scores_img, aspect='auto', cmap=cmap, vmin=0, vmax=1)\n",
    "    # Set up the 'y' label axis\n",
    "    plt.yticks(np.arange(len(label_list)), [l.replace(' ', '\\n') for l in label_list])\n",
    "    # Add horizontal grid lines between labels\n",
    "    for i in range(1, scores_img.shape[0]):\n",
    "        plt.axhline(y=i - 0.5, color='gray', linestyle='--')\n",
    "    plt.grid(axis='y', which='major', color='white', alpha=0)\n",
    "    # Setup the 'x' time axis\n",
    "    n_clips = scores_img.shape[1]\n",
    "    plt.xticks(np.arange(n_clips), [f'Clip {i+1}' for i in range(n_clips)])\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    # Add vertical grid lines between time steps\n",
    "    for j in range(1, n_clips):\n",
    "        plt.axvline(x=j - 0.5, color='gray', linestyle='--')\n",
    "    plt.title(f\"{title} - Sound Event Detections\")\n",
    "    # Add colorbar with a consistent scale from 0 to 1\n",
    "    plt.colorbar(im, ticks=[0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_spectrogram(sound, sr, title, figsize=(12, 4), n_fft=2048, hop_length=256, n_mels=128, cmap='viridis'):\n",
    "    \"\"\"Plots the Mel spectrogram of the audio\"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=sound, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "    librosa.display.specshow(log_mel_spectrogram, sr=sr, hop_length=hop_length, x_axis='time', y_axis='mel', cmap=cmap)\n",
    "    plt.title(f\"{title} - Mel Spectrogram\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Analyze Audio File for Health Sounds"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# This function analyzes an audio file for health-related sounds and generates HeAR embeddings for detected clips\n",
    "def analyze_audio_file(file_path, overlap_percent=50, plot=True):\n",
    "    \"\"\"\n",
    "    Analyze audio file for health-related sounds using the event detector model.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the audio file\n",
    "        overlap_percent: Percentage of overlap between adjacent clips\n",
    "        plot: Whether to show visualization plots\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing detection results and HeAR embeddings\n",
    "    \"\"\"\n",
    "    print(f\"\\nAnalyzing file: {file_path}\")\n",
    "    \n",
    "    # Load and preprocess audio\n",
    "    audio, original_sr = librosa.load(file_path, sr=None, mono=True)\n",
    "    if original_sr != SAMPLE_RATE:\n",
    "        print(f\"Resampling from {original_sr}Hz to {SAMPLE_RATE}Hz\")\n",
    "        audio = librosa.resample(audio, orig_sr=original_sr, target_sr=SAMPLE_RATE)\n",
    "    \n",
    "    # Display audio information\n",
    "    print(f\"Audio duration: {len(audio)/SAMPLE_RATE:.2f} seconds\")\n",
    "    \n",
    "    # Show audio visualization if requested\n",
    "    if plot:\n",
    "        plot_waveform(audio, SAMPLE_RATE, title=f\"Audio Waveform: {os.path.basename(file_path)}\")\n",
    "        display(Audio(data=audio, rate=SAMPLE_RATE))\n",
    "    \n",
    "    # Segment audio into overlapping clips\n",
    "    frame_length = int(CLIP_DURATION * SAMPLE_RATE)\n",
    "    frame_step = int(frame_length * (1 - overlap_percent / 100))\n",
    "    \n",
    "    # Pad audio if shorter than frame_length\n",
    "    if len(audio) < frame_length:\n",
    "        audio = np.pad(audio, (0, frame_length - len(audio)), mode='constant')\n",
    "    \n",
    "    # Create overlapping clips\n",
    "    audio_clip_batch = np.array([audio[i:i+frame_length] for i in range(0, len(audio)-frame_length+1, frame_step)])\n",
    "    print(f\"Number of audio clips in batch: {len(audio_clip_batch)}\")\n",
    "    \n",
    "    # Run event detection on all clips using our simple detector\n",
    "    print(f\"Running event detection on audio clips\")\n",
    "    detection_scores_batch = simple_event_detector(audio_clip_batch)\n",
    "    \n",
    "    # Show detection scores\n",
    "    if plot:\n",
    "        plot_detection_scores(\n",
    "            detection_scores_batch, \n",
    "            LABEL_LIST, \n",
    "            title=f'Event Detection: {os.path.basename(file_path)}'\n",
    "        )\n",
    "    \n",
    "    # Health-related labels to focus on\n",
    "    health_labels = ['Cough', 'Snore', 'Baby Cough', 'Breathe', 'Sneeze', 'Throat Clear']\n",
    "    \n",
    "    # Find clips with health-related sounds\n",
    "    health_clips = []\n",
    "    health_clip_indices = []\n",
    "    health_detections = []\n",
    "    \n",
    "    for clip_idx, scores in enumerate(detection_scores_batch):\n",
    "        clip_detections = {}\n",
    "        for label_idx, label in enumerate(LABEL_LIST):\n",
    "            if label in health_labels and scores[label_idx] > DETECTION_THRESHOLD:\n",
    "                if not clip_detections:\n",
    "                    health_clips.append(audio_clip_batch[clip_idx])\n",
    "                    health_clip_indices.append(clip_idx)\n",
    "                clip_detections[label] = float(scores[label_idx])\n",
    "        \n",
    "        if clip_detections:\n",
    "            health_detections.append(clip_detections)\n",
    "    \n",
    "    # Generate HeAR embeddings for clips with health sounds\n",
    "    if health_clips:\n",
    "        print(f\"Found {len(health_clips)} clips with health-related sounds\")\n",
    "        \n",
    "        # Generate embeddings for health clips using the PyTorch HeAR model\n",
    "        print(\"Generating HeAR embeddings for detected health clips\")\n",
    "        hear_embedding_batch = []\n",
    "        \n",
    "        for clip in health_clips:\n",
    "            # Process each clip and get embedding\n",
    "            embedding = process_audio_with_hear(clip).numpy()\n",
    "            hear_embedding_batch.append(embedding.squeeze())\n",
    "            \n",
    "        hear_embedding_batch = np.array(hear_embedding_batch)\n",
    "        \n",
    "        # Create results dictionary with detailed information\n",
    "        results = {\n",
    "            \"file_name\": os.path.basename(file_path),\n",
    "            \"total_clips\": len(audio_clip_batch),\n",
    "            \"health_clips_count\": len(health_clips),\n",
    "            \"health_clip_indices\": health_clip_indices,\n",
    "            \"health_detections\": health_detections,\n",
    "            \"hear_embeddings\": hear_embedding_batch,\n",
    "            \"all_detection_scores\": detection_scores_batch\n",
    "        }\n",
    "        \n",
    "        print(f\"Analysis complete. Found {len(health_clips)} clips with health sounds.\")\n",
    "        return results\n",
    "    \n",
    "    else:\n",
    "        print(\"No health-related sounds detected in this audio file.\")\n",
    "        return {\n",
    "            \"file_name\": os.path.basename(file_path),\n",
    "            \"total_clips\": len(audio_clip_batch),\n",
    "            \"health_clips_count\": 0,\n",
    "            \"all_detection_scores\": detection_scores_batch\n",
    "        }\n",
    "\n",
    "# Analyze the example cough audio\n",
    "results = analyze_audio_file(\"Woman_coughing_three_times.wav\")\n",
    "\n",
    "# If health clips were found, show embeddings information\n",
    "if results[\"health_clips_count\"] > 0:\n",
    "    print(f\"\\nGenerated {results['health_clips_count']} HeAR embeddings\")\n",
    "    print(f\"Embedding shape: {results['hear_embeddings'].shape}\")\n",
    "    \n",
    "    # Plot the first embedding as an example\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(results['hear_embeddings'][0])\n",
    "    plt.title('First Health Sound Embedding Vector')\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Value')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Create and Compare Healthy vs. Sick Audio Files\n",
    "\n",
    "Let's simulate having both healthy and unhealthy audio examples to compare\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load or create a healthy audio example\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Generate or download a healthy audio example\n",
    "def get_healthy_audio():\n",
    "    healthy_file = \"healthy.wav\"\n",
    "    if not os.path.exists(healthy_file):\n",
    "        print(\"No existing healthy audio found. Creating synthetic example.\")\n",
    "        # Create a simple audio breathing sound (or download another example)\n",
    "        sample_rate = 16000\n",
    "        duration = 2  # seconds\n",
    "        t = np.linspace(0, duration, int(sample_rate*duration), endpoint=False)\n",
    "        # Basic breathing simulation - ambient noise with subtle periodic component\n",
    "        noise = np.random.normal(0, 0.05, len(t))\n",
    "        breathing = 0.2 * np.sin(2 * np.pi * 0.5 * t) * np.sin(2 * np.pi * 0.2 * t)\n",
    "        audio = noise + breathing\n",
    "        audio = audio / np.max(np.abs(audio)) * 0.5  # Normalize\n",
    "        \n",
    "        # Save the audio\n",
    "        wavfile.write(healthy_file, sample_rate, audio.astype(np.float32))\n",
    "        print(f\"Created synthetic healthy audio: {healthy_file}\")\n",
    "    else:\n",
    "        print(f\"Using existing healthy audio: {healthy_file}\")\n",
    "    \n",
    "    # Load the audio file\n",
    "    sample_rate, audio = wavfile.read(healthy_file)\n",
    "    return audio, sample_rate\n",
    "\n",
    "# Let's get our healthy audio sample\n",
    "healthy_audio, healthy_sr = get_healthy_audio()\n",
    "\n",
    "# Analyze both audio files\n",
    "healthy_results = analyze_audio_file(\"healthy.wav\")\n",
    "\n",
    "# Rename the cough audio for clarity\n",
    "os.rename(\"Woman_coughing_three_times.wav\", \"unhealthy.wav\") if not os.path.exists(\"unhealthy.wav\") else None\n",
    "unhealthy_results = results  # We already analyzed this above\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Simple Health Classification Based on HeAR Embeddings\n",
    "\n",
    "Let's build a simple health classifier based on the HeAR embeddings.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Function to extract features from embeddings\n",
    "def extract_features_from_embeddings(embedding_batch):\n",
    "    \"\"\"\n",
    "    Extract statistical features from embedding batch\n",
    "    \"\"\"\n",
    "    if len(embedding_batch) == 0:\n",
    "        # Return zeros if no embeddings\n",
    "        return np.zeros(512 * 3)\n",
    "    \n",
    "    # Compute statistics across all clips\n",
    "    mean_embedding = np.mean(embedding_batch, axis=0)\n",
    "    max_embedding = np.max(embedding_batch, axis=0)\n",
    "    min_embedding = np.min(embedding_batch, axis=0)\n",
    "    \n",
    "    # Concatenate features\n",
    "    features = np.concatenate([mean_embedding, max_embedding, min_embedding])\n",
    "    return features\n",
    "\n",
    "# Check if we have embeddings for both healthy and unhealthy\n",
    "has_healthy_embeddings = \"hear_embeddings\" in healthy_results\n",
    "has_unhealthy_embeddings = \"hear_embeddings\" in unhealthy_results\n",
    "\n",
    "if has_healthy_embeddings and has_unhealthy_embeddings:\n",
    "    print(\"Building health classifier using HeAR embeddings from both samples\")\n",
    "    \n",
    "    # Extract features\n",
    "    healthy_features = extract_features_from_embeddings(healthy_results[\"hear_embeddings\"])\n",
    "    unhealthy_features = extract_features_from_embeddings(unhealthy_results[\"hear_embeddings\"])\n",
    "    \n",
    "    # Create a small dataset (in a real scenario, you'd need many more examples)\n",
    "    X = np.vstack([healthy_features, unhealthy_features])\n",
    "    y = np.array([0, 1])  # 0 for healthy, 1 for unhealthy\n",
    "    \n",
    "    # Train a simple classifier\n",
    "    clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "    # Make predictions on the same data (for demonstration)\n",
    "    predictions = clf.predict(X)\n",
    "    print(\"Predictions:\", predictions)\n",
    "    print(\"Accuracy:\", accuracy_score(y, predictions))\n",
    "    \n",
    "    print(\"\\nThis is just a demonstration. In a real application, you'd need:\")\n",
    "    print(\"1. Many more examples (hundreds or thousands)\")\n",
    "    print(\"2. Proper train/test split\")\n",
    "    print(\"3. More sophisticated models and validation\")\n",
    "elif not has_healthy_embeddings and has_unhealthy_embeddings:\n",
    "    print(\"No health-related sounds detected in the healthy sample. This is expected!\")\n",
    "    print(\"Only unhealthy embeddings available, can't build a classifier without both classes.\")\n",
    "else:\n",
    "    print(\"Insufficient embeddings to build a classifier.\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Record and Analyze Your Own Audio\n",
    "\n",
    "You can record your own audio to analyze using your microphone.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def record_audio(duration=3, fs=16000, filename=\"recorded_audio.wav\"):\n",
    "    \"\"\"\n",
    "    Record audio from the microphone\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import sounddevice as sd\n",
    "        import soundfile as sf\n",
    "\n",
    "        print(f\"Recording {duration} seconds of audio. Please make a sound...\")\n",
    "        audio = sd.rec(int(duration * fs), samplerate=fs, channels=1)\n",
    "        sd.wait()  # Wait until recording is finished\n",
    "        \n",
    "        # Normalize and save\n",
    "        audio = audio.flatten()  # Make it 1D\n",
    "        sf.write(filename, audio, fs)\n",
    "        print(f\"Recording saved to {filename}\")\n",
    "        return audio, fs\n",
    "    except Exception as e:\n",
    "        print(f\"Error recording audio: {e}\")\n",
    "        print(\"You need to install sounddevice: pip install sounddevice soundfile\")\n",
    "        return None, None\n",
    "    \n",
    "# Uncomment these lines to record your own audio\n",
    "\"\"\"\n",
    "my_audio, my_sr = record_audio(duration=3, filename=\"my_audio.wav\")\n",
    "if my_audio is not None:\n",
    "    my_results = analyze_audio_file(\"my_audio.wav\")\n",
    "    \n",
    "    # Check if any health sounds were detected\n",
    "    if my_results[\"health_clips_count\"] > 0:\n",
    "        # Extract features and classify\n",
    "        my_features = extract_features_from_embeddings(my_results[\"hear_embeddings\"])\n",
    "        my_prediction = clf.predict([my_features])[0]\n",
    "        print(f\"Health prediction: {'Unhealthy' if my_prediction == 1 else 'Healthy'}\")\n",
    "    else:\n",
    "        print(\"No health-related sounds detected in your recording.\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we've demonstrated:\n",
    "\n",
    "1. Loading the HeAR model and using it to generate embeddings for health-related sounds\n",
    "2. Detecting health sounds (cough, breathe, etc.) in audio clips\n",
    "3. Building a simple health classifier based on HeAR embeddings\n",
    "\n",
    "**Important Limitations:**\n",
    "- This is a simplified demonstration and not a validated health assessment tool\n",
    "- The event detector is a simplified proxy for the actual HeAR event detector\n",
    "- A real classifier would need much more training data\n",
    "- Audio quality, background noise, and recording conditions matter\n",
    "\n",
    "**Next Steps:**\n",
    "- Train with more diverse audio samples\n",
    "- Implement a proper event detector model\n",
    "- Develop temporal analysis of health sounds over time\n",
    "- Combine with other health indicators for better accuracy\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "quick_start_with_hugging_face_pytorch.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
